{"cells":[{"cell_type":"markdown","source":["# **FINAL_SUBMISSION**"],"metadata":{"id":"QnI6uEAIedIr","colab_type":"text"}},{"cell_type":"markdown","source":["**THE CHALLANGE**\n\n\nHelp find ways to improve the performance of machine learning and predictive models by filling in gaps in the datasets prior to model training."],"metadata":{"id":"2W1lvz8k921d","colab_type":"text"}},{"cell_type":"markdown","source":["ANATOMY OF THE CHALLANGE:\n\n\nDO WHAT?\n\nImprove performance of Machine Learning (ML) models by collecting a complete and continuous sensor data stream.\n\nWHY DID IT HAPPEN?\n\n*  Sensor issues or signal noise due to experimental environment/setup\n*  Corrupted of data\n*  Loss of data during transmission (also due to limited bandwidth of transmission)\n*  Interference\n*  Limited amount of power for data collection and transmission\n\n\nWHAT IT DOES?\n\n*  Limits the ability to train accurate ML models to predict features/characteristics in data, which in turn renders the data \"useless\"\n*  Hinders the collection of good-quality data silos\n\n\nHOW TO SOLVE/OBJECTIVE?\n\n*  By \"filling in\" the missing datapoints in the datasets\n*  By \"generating\" the missing datapoints in the datasets\n*  By eliminating/removing the noisy/corrupted information that is embedded in individual datapoints\n\n\n\nDO IT WHEN?\n\n*  Prior to training, i.e. during data cleaning and preprocessing."],"metadata":{"id":"fDm8rIs0Jr7O","colab_type":"text"}},{"cell_type":"markdown","source":["We started by investigating the reasons behind data loss when the data is acquired through a sensor or sensor array. In addition, we also started doing research finding the reasons behind the loss.\n\nOur research concluded that data loss in any dataset does not only occur due to missing data (be it discreet or continuous/timeseries) but also due to incomplete or corrupted or noisy collection of these data that are acquired by the sensors due to the reasons mentioned above.\n\n\n\n---\n\n\n\nHYPOTHESIS:\n\n\n\nWe propose an end-to-end Machine learning pipeline to -fill in the missing data using Generative modeling which involves using a model to generate new examples that plausibly come from an existing distribution of samples.\n\nStacked Denoising Autoencoder for when the sensor data is corrupted or there is a bit of noise in it, we call this type of data noisy data.\nTo obtain proper information about the data, we want Denoising.\nWe define our autoencoder to remove (if not all)most of the noise our data.\n\nTransforms the input into a lower dimensional representation, and a decoder, which tries to reconstruct the original input from the lower dimensional representation. Therefore, these models present some some sort of “bottle neck” in the middle that forces the network to learn how to compress the data in a lower dimensional space. When training these algorithms, the objective is to be able to reconstruct the original input with the minimum amount of information loss. Once the model is trained, we can compress data at will by only using the encoder component of the autoencoder.\n\n\n---\n**(A)**\n\nDETAILS:\nOne model is called the “generator” or “generative network” model that learns to generate new plausible samples. The other model is called the “discriminator” or “discriminative network” and learns to differentiate generated examples from real examples.\n\nThe two models are set up in a contest or a game (in a game theory sense) where the generator model seeks to fool the discriminator model, and the discriminator is provided with both examples of real and generated samples.\n\nAfter training, the generative model can then be used to create new plausible samples on demand.\n\n----\n\n\n**(B)**\n\nAn autoencoder is a neural network used for dimensionality reduction; that is, for feature selection and extraction. Autoencoders with more hidden layers than inputs run the risk of learning the identity function – where the output simply equals the input – thereby becoming useless.\n\nDenoising autoencoders are an extension of the basic autoencoder, and represent a stochastic version of it. Denoising autoencoders attempt to address identity-function risk by randomly corrupting input (i.e. introducing noise) that the autoencoder must then reconstruct, or denoise.\n\nStacked Denoising Autoencoder\n\nA stacked denoising autoencoder is simply many denoising autoencoders strung together.\n\nA key function of SDAs, and deep learning more generally, is unsupervised pre-training, layer by layer, as input is fed through. Once each layer is pre-trained to conduct feature selection and extraction on the input from the preceding layer, a second stage of supervised fine-tuning can follow.\n\nA word on stochastic corruption in SDAs: Denoising autoencoders shuffle data around and learn about that data by attempting to reconstruct it. The act of shuffling is the noise, and the job of the network is to recognize the features within the noise that will allow it to classify the input. When a network is being trained, it generates a model, and measures the distance between that model and the benchmark through a loss function. Its attempts to minimize the loss function involve resampling the shuffled inputs and re-reconstructing the data, until it finds those inputs which bring its model closest to what it has been told is true.\n\n---\n\n**(C)**\n\nEncoder network: It translates the original high-dimension input into the latent low-dimensional code. The input size is larger than the output size.\nDecoder network: The decoder network recovers the data from the code, likely with larger and larger output layers.\n\nThe encoder network essentially accomplishes the dimensionality reduction, just like how we would use Principal Component Analysis (PCA) or Matrix Factorization (MF) for. In addition, the autoencoder is explicitly optimized for the data reconstruction from the code.\n\n\n\n---\n\n**(D)**\n\nDisentangled Variational autoencoders\n\nThe idea of Variational Autoencoder  is actually less similar to all the autoencoder models above, but deeply rooted in the methods of variational bayesian and graphical model.\nInstead of mapping the input into a fixed vector, we want to map it into a distribution.\nIf each variable in the inferred latent representation is only sensitive to one single generative factor and relatively invariant to other factors, we will say this representation is disentangled or factorized. One benefit that often comes with disentangled representation is good interpretability and easy generalization to a variety of tasks.\n\nFor example, a model trained on photos of human faces might capture the gentle, skin color, hair color, hair length, emotion, whether wearing a pair of glasses and many other relatively independent factors in separate dimensions. Such a disentangled representation is very beneficial to facial image generation.\n\n\nhttps://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#beta-vae"],"metadata":{"id":"Clp6WdehRgm_","colab_type":"text"}},{"cell_type":"markdown","source":["# **Proof of Concept #3**"],"metadata":{"id":"IkuHeWXaJTeb","colab_type":"text"}},{"cell_type":"markdown","source":["# **NASA Meteorite Landing dataset: Recovering/approximating/imputing missing values**\n\n# **# Algorithm: Deep Convolutional Generative Adversarial Network (DCGAN)**"],"metadata":{"colab_type":"text","id":"SBwaJJLCJT67"}},{"cell_type":"markdown","source":["# **Our approach**\n\nHere, we are transforming each sample (row) of the Meteorite CSV dataset into an image. The dataset has 45716 rows and 9 columns, including metorites' names and IDs. Our algorithm transforms each row it into a 3x3 image and zero pads the outer periphery of the 3x3 matrix (to make it a 7x7 matrix) so that each row fits into a 2D Convolutional filter. Then, once the training and evaluation is complete, we plan to recover the original data (CSV) from the images generated by the DCGAN.\n\n# **Challenges**\n\nThere are mainly 2 challenges that we faced (#1 and #2) while implementing this demo. #3 is our planned future work.\n\n\n\n1.   The dataset is probably not big enough for a regular Convolutional Neural Network (CNN)-based DCGAN architecture.\n2.   We had to encode all categorical values to numeric values, including the names. The problem here was our label encoder generated different numeric values for the meteorite names compared to their IDs. We chose not to exclude these two columns from the dataset for the sake of architectural simplicity at this moment.\n\n\n# **Future work**\nDue to time constraints, we are yet to recover the generated csv data from the output images and evaluate whether the algorithm converged or not. If it did not converge, then we plan to tune the hyperparameters, modify the CNN architecture if necessary and re-train the DCGAN algorithm."],"metadata":{"id":"JBWlq21JaepW","colab_type":"text"}},{"cell_type":"markdown","source":["### Import TensorFlow and other libraries"],"metadata":{"colab_type":"text","id":"3wPcFbxLJT7B"}},{"cell_type":"code","source":["from __future__ import absolute_import, division, print_function, unicode_literals\ntry:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass"],"metadata":{"colab_type":"code","id":"CytkWIlIJT7H","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ERROR: Line magic function &#96;%tensorflow_version&#96; not found.\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["import tensorflow as tf\ntf.__version__"],"metadata":{"colab_type":"code","outputId":"e4b89705-dcef-4733-c024-9b1df29e0760","id":"Yr_IsI0XJT7S","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3193564547569598&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">import</span> tensorflow <span class=\"ansigreen\">as</span> tf<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> tf<span class=\"ansiyellow\">.</span>__version__<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ImportError</span>: No module named &apos;tensorflow&apos;</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# To generate GIFs\n!pip install imageio"],"metadata":{"colab_type":"code","outputId":"ef6fbde2-1ea7-48ec-d8a4-b166bd1323e4","id":"f2JlVi_aJT7f","colab":{"base_uri":"https://localhost:8080/","height":90}},"outputs":[],"execution_count":11},{"cell_type":"code","source":["import glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport time\nfrom tensorflow.keras import layers\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import MinMaxScaler\nfrom IPython import display"],"metadata":{"colab_type":"code","id":"Lx-gzPmgJT7h","colab":{}},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from azureml.core import Workspace\nfrom azureml.core.run import Run"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Load and prepare NASA Meteorite dataset for **training**"],"metadata":{"colab_type":"text","id":"x4TGihDDJT7k"}},{"cell_type":"code","source":["##### Initialize training dataset #####\n\n# mount google drive location where you saved a .zip archive of your folder that contains images; then unzip the file\n#from google.colab import drive\n#drive.mount('/content/drive')"],"metadata":{"id":"hhULQyuGek0V","colab_type":"code","outputId":"19389bf7-d168-48e4-fc1b-502998c223e9","colab":{"base_uri":"https://localhost:8080/","height":122}},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Enter databricks dbfs\ncd /dbfs"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#import pandas as pd\n#databricks_filepath = \"/dbfs/FileStore/tables/Meteorite_Landings_clean.csv\"\n#df = pd.read_csv(databricks_filepath)\n#df.shape\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">58</span><span class=\"ansired\">]: </span>(45716, 9)</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["**Load dataset and encode categorical values**"],"metadata":{"id":"vClB8Xo-gKA8","colab_type":"text"}},{"cell_type":"code","source":["# Load dataset and encode categorical values\nimport pandas as pd\n#df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/NASA-challenge-sample-datasets/Meteorite_Landings_clean.csv')\ndatabricks_filepath = \"/dbfs/FileStore/tables/Meteorite_Landings_clean.csv\"\ndf = pd.read_csv(databricks_filepath)\ndf_filled = df.fillna(0)\n\n# Encode categorical values\nlabel_enc = preprocessing.LabelEncoder()\ndata_clean = df_filled.apply(lambda series: pd.Series(label_enc.fit_transform(series), index=series.index))\n#data_clean.to_csv('Meteorite_not_normalized.csv',index=True)\n\n# Normalize numeric data only (range: 0-255)\nfirst_2_col = data_clean[data_clean.columns[:2]]\nminmax = MinMaxScaler(feature_range=(0, 255), copy=True)\ndata_norm_minmax = minmax.fit_transform(data_clean[['nametype', 'recclass', 'mass (g)', 'fall', 'year_numeric', 'reclat', 'reclong']])\n\n#data_norm = normalize(data_clean[['nametype', 'recclass', 'mass (g)', 'fall', 'year_numeric', 'reclat', 'reclong']])\n\ndataset_complete = np.hstack((first_2_col, data_norm_minmax))\n#np.savetxt(\"Meteorite_clean_minmax.csv\", dataset_complete, delimiter=\",\")\n"],"metadata":{"id":"sIOmIx7jvLgz","colab_type":"code","colab":{}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["**Start zero padding the data so that it fits into a 2D Convolutional filter**"],"metadata":{"id":"NerOTN2vgEJy","colab_type":"text"}},{"cell_type":"code","source":["## Start zero padding the data so that it fits into a 2D Convolutional filter\nzeros_vert = np.zeros((3, 2))\nzeros_hori = np.zeros((2, 7))\nzeros_vert.shape, zeros_hori.shape, dataset_complete.shape"],"metadata":{"id":"ZEmbmYxLzNUM","colab_type":"code","outputId":"2ebc0cfe-3dff-4945-f74b-8425313eabdd","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Rehsape data and pad zeros to increase dimensionality\ndataset_intermed = dataset_complete.reshape(dataset_complete.shape[0], 3, 3).astype('float32')\ndataset_intermed.shape"],"metadata":{"id":"3Bnkqr58tA9t","colab_type":"code","outputId":"ad95fe80-9618-4d80-a8b6-7e75c534a455","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Pad zeros horizontally\ndataset_hor_pad = []\nfor i in range(dataset_intermed.shape[0]):\n  dataset_hor_pad_1 = np.hstack((zeros_vert, dataset_intermed[i,:,:], zeros_vert))\n  dataset_hor_pad.append(dataset_hor_pad_1)\n\ndataset_hor_pad = np.array(dataset_hor_pad)\ndataset_hor_pad_1.shape, dataset_hor_pad.shape"],"metadata":{"id":"l9qQ9f600Vvj","colab_type":"code","outputId":"bd601fd4-dd5d-4931-a820-c0e689ad3523","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Pad zeros vertically\ndataset_ver_pad = []\nfor i in range(dataset_hor_pad.shape[0]):\n  dataset_ver_pad_1 = np.vstack((zeros_hori, dataset_hor_pad[i,:,:], zeros_hori))\n  dataset_ver_pad.append(dataset_ver_pad_1)\n\ndataset_padded = np.array(dataset_ver_pad)\ndataset_ver_pad_1.shape, dataset_padded.shape"],"metadata":{"id":"Qjtbdh854O2_","colab_type":"code","outputId":"bc3a7972-de71-4047-d008-4e9eb23d5165","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["**Reshape training data, define batch and buffer sizes**"],"metadata":{"id":"3Jbmom_jgbX9","colab_type":"text"}},{"cell_type":"code","source":["# Reshape training data, define batch and buffer sizes\ndataset = dataset_padded.reshape(dataset_padded.shape[0], 7, 7, 1).astype('float32')\n\n# Initialize buffer and batch size\nBUFFER_SIZE = dataset_padded.shape[0]\nBATCH_SIZE = 256\n\n# BATCH and SHUFFLE the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"metadata":{"id":"_iwOatDqmVmV","colab_type":"code","colab":{}},"outputs":[],"execution_count":28},{"cell_type":"code","source":["dataset.shape"],"metadata":{"id":"INhzpXh6F1RP","colab_type":"code","outputId":"36cd4625-1447-426e-d045-eba7e3b487a9","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## Create the models\n\nBoth the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."],"metadata":{"colab_type":"text","id":"WZjMLwFcJT8l"}},{"cell_type":"markdown","source":["### The Generator\n\nThe generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). It starts with a **`Dense` layer that takes this seed as input**, then **upsamples it several times until it reaches the desired image size** of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."],"metadata":{"colab_type":"text","id":"mI-Ne_5_JT8m"}},{"cell_type":"code","source":["# Create generator\n\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((7, 7, 256)))\n    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 7, 7, 1)\n\n    return model"],"metadata":{"id":"WbuMZGRLnaEE","colab_type":"code","colab":{}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["**Use the (as yet untrained) generator to create an image.**"],"metadata":{"colab_type":"text","id":"3UYdT2TdJT8o"}},{"cell_type":"code","source":["generator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0])"],"metadata":{"colab_type":"code","id":"tAGLa53Hq-54","colab":{"base_uri":"https://localhost:8080/","height":286},"cellView":"both","outputId":"3a31c5ff-02c6-43e3-c439-ea93f624a55f"},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### The Discriminator\n\nThe discriminator is a CNN-based image classifier."],"metadata":{"colab_type":"text","id":"D_8k68hjJT8q"}},{"cell_type":"code","source":["# Create a discriminator to police the generator (notice the input shape of the first Conv2D layer)\n\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[7, 7, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model"],"metadata":{"id":"len8EoxnuKS-","colab_type":"code","colab":{}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["**Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images.**"],"metadata":{"colab_type":"text","id":"GW-tP5LoJT8s"}},{"cell_type":"code","source":["discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)"],"metadata":{"colab_type":"code","id":"aKewzWxktMzb","colab":{"base_uri":"https://localhost:8080/","height":35},"cellView":"both","outputId":"1d54c328-847c-4ef3-ad9d-fa44c4ff3718"},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Define the loss and optimizers\n\nDefine loss functions and optimizers for both models."],"metadata":{"colab_type":"text","id":"GHJw0oziJT8u"}},{"cell_type":"code","source":["# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"metadata":{"colab_type":"code","id":"N7pxVNpJJT8u","colab":{}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### Discriminator loss\n\nThis method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."],"metadata":{"colab_type":"text","id":"yS8AY32aJT8x"}},{"cell_type":"code","source":["def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss"],"metadata":{"colab_type":"code","id":"yIuZ1jUYJT8y","colab":{}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["### Generator loss\nThe generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."],"metadata":{"colab_type":"text","id":"ik5qxVj5JT81"}},{"cell_type":"code","source":["def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)"],"metadata":{"colab_type":"code","id":"V29EcGwtJT82","colab":{}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["The discriminator and the generator optimizers are different since we will train two networks separately."],"metadata":{"colab_type":"text","id":"JlWL9N2LJT86"}},{"cell_type":"code","source":["generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"],"metadata":{"colab_type":"code","id":"pwAPk_rhJT87","colab":{}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### Save checkpoints\nSave and restore models, which can be helpful in case a long running training task is interrupted."],"metadata":{"colab_type":"text","id":"abECcSA9JT89"}},{"cell_type":"code","source":["checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)"],"metadata":{"colab_type":"code","id":"R98MWKQ6JT8-","colab":{}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["## Define the training loop"],"metadata":{"colab_type":"text","id":"hEG_qhsoJT9F"}},{"cell_type":"code","source":["EPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])"],"metadata":{"id":"WbOzaGf7wde8","colab_type":"code","colab":{}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."],"metadata":{"colab_type":"text","id":"Chg4RIG-JT9J"}},{"cell_type":"code","source":["# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"],"metadata":{"colab_type":"code","id":"Sfrgc2CsJT9K","colab":{}},"outputs":[],"execution_count":52},{"cell_type":"code","source":["def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)"],"metadata":{"colab_type":"code","id":"X5bgFv7dJT9N","colab":{}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["**Generate and save images**"],"metadata":{"colab_type":"text","id":"9y0WxdT5JT9Q"}},{"cell_type":"code","source":["def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n  print('prediction shape = ', predictions.shape)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.imshow(predictions[i, :, :, 0])\n      plt.axis('off')\n\n  #plt.imshow(predictions[0, :, :, 0])\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()"],"metadata":{"colab_type":"code","id":"hSm77QfuJT9R","colab":{}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["## Train the model\nCall the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n\nAt the beginning of the training, the generated image tiles will look like random noise. As training progresses, the tiles will look increasingly real This is the expectation when enough training data i.e. images, is provided. Usually with only a few images as input, the discriminator doesn't get the oppooertunity to police the generator properly."],"metadata":{"colab_type":"text","id":"hVkUZNCAJT9S"}},{"cell_type":"code","source":["%%time\ntrain(train_dataset, EPOCHS)"],"metadata":{"id":"Nw4iLv21vSOi","colab_type":"code","outputId":"d8b0ddd4-8017-478b-d3ab-004a3af2194b","colab":{"base_uri":"https://localhost:8080/","height":1000}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Restore the latest checkpoint."],"metadata":{"colab_type":"text","id":"2u5VLnPxJT9U"}},{"cell_type":"code","source":["checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"metadata":{"colab_type":"code","id":"oDx32LnJJT9U","colab":{}},"outputs":[],"execution_count":59}],"metadata":{"colab":{"name":"DEDOMENA_meteorite_dcgan.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"name":"DEDOMENA_meteorite_dcgan","notebookId":3193564547569588},"nbformat":4,"nbformat_minor":0}
